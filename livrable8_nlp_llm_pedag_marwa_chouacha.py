# -*- coding: utf-8 -*-
"""Livrable8_NLP_LLM_Pedag_MARWA_CHOUACHA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zx-CvuclFHbEuI3cEy6ZcggJR-d607lD

#  NLP moderne, Transformers et Mini-Chatbot RAG (Notebook p√©dagogique)

Ce notebook a pour objectif de te faire passer **du NLP "classique"** (sacs de mots, TF-IDF, r√©gression logistique, SVM)
√† un **NLP moderne bas√© sur les Transformers**, les **embeddings** et le **RAG (Retrieval-Augmented Generation)**.

---

##  Objectifs p√©dagogiques

√Ä la fin de ce notebook, tu seras capable de :

1. **Expliquer** la diff√©rence entre :
   - repr√©sentations classiques (bag-of-words, TF-IDF),
   - et **embeddings continus** produits par des mod√®les pr√©-entra√Æn√©s.

2. **Utiliser** un mod√®le de type Transformer pour :
   - calculer des **embeddings de phrases**,
   - mesurer la **similarit√© s√©mantique** entre textes.

3. **Construire un mini syst√®me de retrieval** :
   - indexer des documents sous forme d'embeddings,
   - retrouver les passages les plus pertinents pour une question.

4. **Mettre en place un mini syst√®me RAG** :
   - r√©cup√©rer les passages pertinents,
   - les injecter dans le prompt d'un mod√®le g√©n√©ratif,
   - obtenir une r√©ponse qui s'appuie sur le contenu des documents.

5. **Construire un mini chatbot RAG** (version simple) :
   - poser plusieurs questions,
   - garder un petit historique de la conversation,
   - r√©pondre en s‚Äôappuyant sur les documents.

---

##  Pr√©requis

Tu as d√©j√† vu :
- les bases du **NLP classique** (tokenisation, sac de mots, TF-IDF),
- la **classification de textes** (ex : classification de sentiments),
- quelques notions de **deep learning** (r√©seaux de neurones de base).

Nous allons maintenant **monter en abstraction** :
- utiliser des **mod√®les pr√©-entra√Æn√©s** (Transformers),
- les employer comme **extracteurs d'embeddings**,
- puis construire par-dessus un syst√®me de **retrieval + g√©n√©ration**.

---

 **Organisation du notebook :**

1. Rappel rapide : limites du NLP classique  
2. Embeddings modernes de phrases  
3. Intuition des Transformers  
4. Retrieval s√©mantique  
5. Mini-RAG (question ‚Üí documents ‚Üí r√©ponse)  
6. Mini-chatbot RAG

## 0.1 ‚Äî Rappel : NLP "classique" et ses limites

Dans le cours pr√©c√©dent, tu as d√©j√† manipul√© :

- **Repr√©sentations discr√®tes** :
  - Sac de mots (Bag-of-Words),
  - TF-IDF.
- **Classifieurs "classiques"** :
  - R√©gression logistique,
  - SVM, Random Forest, etc.
- √âventuellement un peu de **deep learning "simple"** (MLP, LSTM de base).

Ces approches ont permis de traiter des t√¢ches comme :
- la **classification de sentiments** (positif / n√©gatif),
- la d√©tection de **spams**,
- la classification de sujets (sport, politique, etc.).

---

###  Limites de ces approches

1. **Perte de l'ordre des mots**  
   Sac de mots / TF-IDF ignorent l'ordre :
   - "Le chien mord l'homme"  
   - "L'homme mord le chien"  
   ‚Üí m√™me vocabulaire, signification tr√®s diff√©rente.

2. **Vecteurs tr√®s grands et tr√®s clairsem√©s (sparse)**  
   - Une dimension par mot du vocabulaire,
   - Beaucoup de z√©ros,
   - Peu de g√©n√©ralisation entre mots proches (ex : *bon*, *excellent*).

3. **Peu de compr√©hension s√©mantique**  
   - Deux phrases avec le m√™me sens mais des mots diff√©rents peuvent √™tre tr√®s √©loign√©es dans l'espace TF-IDF.
   - Exemple :  
     - "Ce film est incroyable."  
     - "J'ai ador√© ce long-m√©trage."  
     ‚Üí vocabulaire diff√©rent, m√™me sentiment.

4. **Difficile √† utiliser pour des t√¢ches plus complexes**  
   - R√©sum√© automatique,
   - Question-R√©ponse,
   - Chatbot "intelligent" qui s'appuie sur des documents.

---

###  Id√©e cl√© du NLP moderne

Au lieu de repr√©senter un texte comme un vecteur sparse bas√© sur le vocabulaire,
on va le repr√©senter comme un **vecteur dense de dimension mod√©r√©e** (par ex. 384, 768 ou 1024).

Ce vecteur, appel√© **embedding**, est appris par un **mod√®le pr√©-entra√Æn√©** (souvent un Transformer)
sur des **milliards de phrases**.

**Propri√©t√© magique :**  
Des phrases **proches en sens** ‚Üí **proches en embedding** (distance / cosinus faible).  
Des phrases **tr√®s diff√©rentes** ‚Üí **embeddings √©loign√©s**.

C'est cette id√©e que nous allons exploiter pour :
- faire du **retrieval s√©mantique**,
- puis construire un **mini syst√®me RAG / chatbot**.
"""

# ============================================================
# 0.2 ‚Äî Installation des biblioth√®ques n√©cessaires
# √Ä ex√©cuter une seule fois au d√©but du notebook (Colab)
# ============================================================

!pip install -q transformers datasets sentence-transformers accelerate faiss-cpu

import torch
import transformers
import datasets
from sentence_transformers import SentenceTransformer

print(" PyTorch version :", torch.__version__)
print(" Transformers version :", transformers.__version__)
print(" Datasets version :", datasets.__version__)

# V√©rif rapide du GPU (optionnel, mais motivant pour les √©tudiants)
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(" GPU disponible :", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print(" Pas de GPU d√©tect√©, on reste sur CPU.")

"""### 0.3 ‚Äî Commentaire sur la cellule d'installation

Dans la cellule pr√©c√©dente, nous avons :

1. **Install√© les biblioth√®ques n√©cessaires** :
   - `transformers` : la librairie Hugging Face pour manipuler les mod√®les Transformers.
   - `datasets` : pour charger facilement des jeux de donn√©es NLP.
   - `sentence-transformers` : pour obtenir des **embeddings de phrases** de haute qualit√©.
   - `accelerate` : pour faciliter l'utilisation de GPU / multi-device (utile plus tard).
   - `faiss-cpu` : pour faire de la **recherche de voisins proches** (retrieval rapide).

2. **Import√© les modules principaux** dont nous aurons besoin.

3. **V√©rifi√© la pr√©sence d'un GPU** :
   - Si un GPU est disponible dans Colab, on l'utilisera pour acc√©l√©rer les calculs.
   - Sinon, le notebook fonctionne aussi sur CPU, mais plus lentement.

√Ä partir de maintenant, nous allons construire pas √† pas :
- un **mod√®le d'embeddings de phrases**,  
- un **moteur de recherche s√©mantique**,  
- puis un **mini syst√®me RAG / chatbot**.

## 1. Embeddings de phrases : de quoi parle-t-on ?

L'id√©e des **embeddings** est de repr√©senter un texte (mot, phrase, paragraphe)
par un **vecteur dense** de taille fixe, par exemple de dimension 384, 768, etc.

Ce vecteur est construit par un **mod√®le pr√©-entra√Æn√©** (souvent un Transformer)
sur des millions ou milliards de phrases.  
### R√©sultat : des phrases **proches en sens** ont des vecteurs **proches** dans cet espace.

---

### 1.1 ‚Äî Mod√®le que nous allons utiliser

Nous allons utiliser un mod√®le tr√®s populaire de la biblioth√®que
[`sentence-transformers`](https://www.sbert.net) :

- **Nom** : `sentence-transformers/all-MiniLM-L6-v2`
- Propri√©t√©s :
  - Taille raisonnable (rapide sur CPU / GPU),
  - Produit des embeddings de dimension 384,
  - Bien adapt√© pour la **similarit√© s√©mantique de phrases**.

---

### 1.2 ‚Äî Exp√©rience simple

Nous allons :

1. D√©finir une petite liste de phrases en fran√ßais.
2. Calculer leurs embeddings avec le mod√®le.
3. Calculer une **matrice de similarit√© cosinus** entre toutes les phrases.
4. Observer :
   - Les phrases qui ont un sens proche ont une similarit√© cosinus √©lev√©e (proche de 1).
   - Les phrases tr√®s diff√©rentes ont une similarit√© plus faible, voire n√©gative.

Cette exp√©rience est la **brique de base** pour :

- la recherche de documents proches d'une requ√™te (**retrieval**),
- le regroupement de phrases,
- et plus tard, notre **mini-RAG / chatbot**.
"""

# ============================================================
# 1.3 ‚Äî Premi√®re exp√©rience : embeddings & similarit√© cosinus
# ============================================================

from sentence_transformers import SentenceTransformer
import numpy as np

# 1) Charger le mod√®le de sentence embeddings
model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedder = SentenceTransformer(model_name)
print(f" Mod√®le charg√© : {model_name}")

# 2) D√©finir quelques phrases de test (en fran√ßais)
sentences = [
    "Ce film √©tait incroyable, j'ai ador√© chaque minute.",
    "J'ai vraiment aim√© ce film, il √©tait excellent.",
    "Ce film est une perte de temps, je le d√©teste.",
    "Il fait tr√®s beau aujourd'hui, le soleil brille.",
    "Le temps est magnifique, il y a beaucoup de soleil.",
    "Je dois acheter du pain √† la boulangerie."
]

print("\n Phrases test :")
for i, s in enumerate(sentences):
    print(f"{i}: {s}")

# 3) Calculer les embeddings (matrice [nb_phrases, dim_embedding])
embeddings = embedder.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)
print("\n Forme de la matrice d'embeddings :", embeddings.shape)

# 4) Fonction de similarit√© cosinus
def cosine_similarity_matrix(X):
    """
    X : matrice (n, d) o√π n = nb de phrases, d = dimension embedding
    Retourne une matrice (n, n) des similarit√©s cosinus.
    """
    # X est d√©j√† normalis√© (norme 1) => produit scalaire = cosinus
    return np.matmul(X, X.T)

sim_matrix = cosine_similarity_matrix(embeddings)

# 5) Afficher la matrice de similarit√© de mani√®re lisible
np.set_printoptions(precision=2, suppress=True)
print("\n Matrice de similarit√© cosinus entre phrases :\n")
print(sim_matrix)

# 6) Afficher quelques similarit√©s cibl√©es
def show_similarity(i, j):
    print(f"\nSim({i}, {j}) = cosinus( phrase[{i}], phrase[{j}] ) = {sim_matrix[i, j]:.3f}")
    print(f"  phrase[{i}] : {sentences[i]}")
    print(f"  phrase[{j}] : {sentences[j]}")

# Phrases tr√®s proches en sens (film positif)
show_similarity(0, 1)

# Film positif vs film n√©gatif
show_similarity(0, 2)

# Deux phrases sur la m√©t√©o
show_similarity(3, 4)

# Phrase sur la m√©t√©o vs phrase sur la boulangerie
show_similarity(3, 5)

"""## 1.4 ‚Äî Interpr√©tation des similarit√©s cosinus

La cellule pr√©c√©dente a affich√© :

1. Une **matrice de similarit√© cosinus** `sim_matrix` de taille `(n, n)`  
   - `sim_matrix[i, j]` = similarit√© cosinus entre la phrase `i` et la phrase `j`.  
   - La diagonale vaut **1.0** (une phrase est parfaitement similaire √† elle-m√™me).

2. Quelques similarit√©s cibl√©es, par exemple :
   - `Sim(0, 1)` : deux phrases exprimant un **avis positif** sur un film.
   - `Sim(0, 2)` : phrase positive vs phrase **n√©gative** sur un film.
   - `Sim(3, 4)` : deux phrases sur la **m√©t√©o ensoleill√©e**.
   - `Sim(3, 5)` : m√©t√©o vs **boulangerie**.

---

### Comment interpr√©ter les valeurs ?

- La similarit√© cosinus est comprise entre **-1** et **1** :
  - **1.0** ‚Üí vecteurs identiques (m√™me direction),
  - **0.0** ‚Üí vecteurs orthogonaux (aucune corr√©lation),
  - **-1.0** ‚Üí vecteurs oppos√©s (tr√®s "contraires").

En pratique, pour les embeddings de phrases :

- **Entre 0.6 et 1.0** : phrases souvent **proches en sens**.
- **Entre 0.3 et 0.6** : phrases parfois li√©es, parfois diff√©rentes.
- **En dessous de 0.3** : phrases plut√¥t **sans rapport**.
- Valeurs vraiment n√©gatives : assez rares mais possibles, indiquant une **opposition forte** dans l'espace d'embeddings.

---

### Ce que tu dois observer ici

1. `Sim(0, 1)` devrait √™tre **√©lev√©e**  
   ‚Üí les deux phrases expriment un avis positif sur un film.

2. `Sim(0, 2)` devrait √™tre **plus faible**, voire nettement plus basse  
   ‚Üí positif vs n√©gatif.

3. `Sim(3, 4)` devrait √™tre **√©lev√©e**  
   ‚Üí m√™me sujet (m√©t√©o, soleil).

4. `Sim(3, 5)` devrait √™tre **plus faible**  
   ‚Üí m√©t√©o vs boulangerie (th√®mes diff√©rents).

---

###  Mini-exercice

- Ajoute tes **propres phrases** dans la liste `sentences`.
- Re-ex√©cute la cellule de calcul des embeddings et de la matrice de similarit√©.
- Observe :
  - quelles paires de phrases ont la plus grande similarit√© ?
  - quelles paires sont les plus √©loign√©es ?

 **Id√©e importante :**  
Nous venons de construire la brique de base pour :
- **retrouver les phrases les plus proches** d'une requ√™te,
- ce qui sera au c≈ìur de notre **moteur de retrieval s√©mantique**.

#**Mini moteur de recherche s√©mantique (Retrieval)**

# 2. Retrieval S√©mantique ‚Äî **Trouver les passages les plus proches d'une question**

Les embeddings de phrases nous permettent de calculer *la similarit√© s√©mantique* entre deux textes.

Nous allons maintenant utiliser cette id√©e pour construire un **mini moteur de recherche** :

#QUESTION ‚Üí embedding ‚Üí comparaison ‚Üí top-k passages

C'est exactement ce que font :
- les syst√®mes de **RAG (Retrieval-Augmented Generation)**,
- les assistants type **ChatGPT** quand ils s'appuient sur des documents,
- les moteurs de recherche modernes.

---

##  Id√©e cl√©

Un moteur de recherche s√©mantique =  
**encoder les documents** + **encoder la requ√™te** + **mesurer les similarit√©s cosinus**.

Contrairement √† la recherche par mots-cl√©s :

- si tu cherches *"voiture √©lectrique"*  
  ‚Üí il peut te proposer un passage contenant *"v√©hicule √† batterie"*

- il comprend le **sens**, pas seulement le vocabulaire.

---

## Ce que nous allons faire maintenant

1. Construire une petite base de **paragraphes** (mini-wiki).
2. Calculer les embeddings de tous les paragraphes.
3. Impl√©menter une fonction `search(query, top_k=3)` :
   - Encode la requ√™te
   - Calcule les similarit√©s
   - Retourne les meilleurs passages.

Puis on testera avec des requ√™tes :

- "Comment entra√Æner un mod√®le ?"  
- "Qu'est-ce que le NLP ?"  
- "C'est quoi un Transformer ?"  

Et on verra que √ßa marche d√©j√† tr√®s tr√®s bien.

#Construire une mini base documentaire
"""

# ============================================================
# 2.1 ‚Äî Mini base documentaire (passages)
# ============================================================

documents = [
    """Le traitement automatique du langage naturel (NLP)
    est un domaine de l'intelligence artificielle qui vise
    √† permettre aux machines de comprendre et g√©n√©rer du texte.""",

    """Les mod√®les Transformers sont bas√©s sur le m√©canisme
    d'attention, qui permet au mod√®le de se concentrer sur
    les parties importantes d'une s√©quence.""",

    """L'apprentissage profond a r√©volutionn√© le NLP moderne
    gr√¢ce √† des mod√®les entra√Æn√©s sur de tr√®s grands corpus
    de textes.""",

    """Pour entra√Æner un mod√®le de classification de texte,
    on pr√©pare un dataset contenant des phrases et leurs
    √©tiquettes, puis on optimise la perte sur des mini-batchs.""",

    """Les embeddings de phrases permettent de repr√©senter un
    texte sous forme de vecteur dense, capturant le sens global
    de la phrase.""",

    """Le RAG (Retrieval-Augmented Generation) combine
    r√©cup√©ration de documents et g√©n√©ration de texte pour
    fournir des r√©ponses contextualis√©es √† partir d'une base
    de connaissances.""",

    """Un chatbot intelligent utilise souvent un mod√®le
    g√©n√©ratif et peut consulter une base documentaire pour
    r√©pondre de fa√ßon plus fiable et pr√©cise."""
]

print(f"  Longueur du document : {len(documents)}")

"""##Encoder tous les documents"""

# ============================================================
# 2.2 ‚Äî Encoder les documents
# ============================================================

# On r√©utilise le mod√®le embedder charg√© dans S1
doc_embeddings = embedder.encode(
    documents,
    convert_to_numpy=True,
    normalize_embeddings=True
)

print("Embeddings documents :", doc_embeddings.shape)

"""##Fonction de recherche s√©mantique"""

# ============================================================
# 2.3 ‚Äî Fonction de recherche s√©mantique
# ============================================================

def search(query, top_k=3):
    """
    Retourne les top_k documents les plus proches de la requ√™te.
    """
    # Encoder la requ√™te
    query_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]

    # Calcul des similarit√©s cosinus (produit scalaire)
    similarities = np.dot(doc_embeddings, query_emb)

    # Indices des meilleurs passages
    best_idx = np.argsort(similarities)[::-1][:top_k]

    results = []
    for idx in best_idx:
        results.append({
            "document_index": idx,
            "score": float(similarities[idx]),
            "content": documents[idx].strip()
        })
    return results

print(" Fonction de recherche s√©mantique pr√™te.")

"""##Tests du moteur de recherche s√©mantique"""

# ============================================================
# 2.4 ‚Äî Tests du moteur de recherche s√©mantique
# ============================================================

queries = [
    "C'est quoi le NLP ?",
    "Comment fonctionne un Transformer ?",
    "Comment entra√Æner un mod√®le de classification ?",
    "Explique le RAG.",
    "Qu'est-ce qu'un chatbot ?"
]

for q in queries:
    print("\n==============================")
    print(" Requ√™te :", q)
    results = search(q, top_k=2)
    for r in results:
        print(f"\n Score : {r['score']:.3f}")
        print(f" Passage : {r['content']}")

"""##Explication + interpr√©tation

# 2.5 ‚Äî Analyse des r√©sultats

Nous venons de construire un moteur de recherche s√©mantique complet,
avec moins de **30 lignes de code** :

1. Les documents ont √©t√© encod√©s sous forme de **vecteurs d'embeddings**.
2. Une requ√™te est elle aussi transform√©e en embedding.
3. On calcule la similarit√© cosinus entre la requ√™te et tous les documents.
4. On trie, on renvoie les `top_k` passages.

---

##  Ce qu'il faut remarquer

### 1) Pas besoin de mots-cl√©s exacts  
La requ√™te :  
> "Explique le RAG."  
retrouve un passage contenant  
> "Retrieval-Augmented Generation".

### 2) Robustesse aux reformulations humaines  
> "Comment fonctionne un Transformer ?"  
retrouve un passage parlant du m√©canisme d'attention.

### 3) Distance s√©mantique r√©elle  
> "Qu'est-ce qu'un chatbot ?"  
retrouve un passage li√© aux agents conversationnels.

---

##  Pourquoi √ßa marche si bien ?

Parce que les embeddings de phrases sont entra√Æn√©s sur **des milliards de textes**
et apprennent une g√©om√©trie o√π :

- les concepts proches ‚Üí vecteurs proches  
- les concepts diff√©rents ‚Üí vecteurs √©loign√©s  

C‚Äôest ce qui permet √† nos futurs syst√®mes RAG : ***"Question ‚Üí documents ‚Üí g√©n√©ration de r√©ponse"***  de fonctionner en pratique

#**Mini RAG : Retrieval-Augmented Generation**

##Intuition du RAG

# 3. RAG ‚Äî Retrieval-Augmented Generation

Jusqu'ici, nous avons :

1. Vu comment produire des **embeddings de phrases**.
2. Construit un **moteur de recherche s√©mantique** (retrieval) sur une mini base de documents.

Prochaine √©tape : **g√©n√©rer une r√©ponse** √† partir des passages retrouv√©s.

---

## 3.1 ‚Äî Id√©e du RAG

Un syst√®me **RAG (Retrieval-Augmented Generation)** suit en g√©n√©ral ce sch√©ma :

1. L'utilisateur pose une **question**.
2. On **r√©cup√®re** (retrieve) les passages les plus pertinents dans une base documentaire.
3. On construit un **prompt** du type :

> "En te basant STRICTEMENT sur les documents suivants :  
> [DOC 1]  
> [DOC 2]  
> R√©ponds √† la question : [QUESTION]"

4. On envoie ce prompt √† un **mod√®le g√©n√©ratif** (ex : T5, GPT-like, etc.).
5. Le mod√®le g√©n√®re une **r√©ponse textuelle** qui s'appuie *normalement* sur les documents.

---

## 3.2 ‚Äî Pourquoi c'est puissant ?

- Le mod√®le n'a pas besoin de tout **m√©moriser en param√®tres** :
  - les infos sp√©cifiques sont dans une base externe (documents, wiki, base de cours‚Ä¶).
- On peut **mettre √† jour la base** sans r√©entra√Æner le mod√®le.
- On garde une meilleure **tra√ßabilit√©** :
  - on peut montrer quels passages ont servi √† r√©pondre.

---

## 3.3 ‚Äî Ce que nous allons faire maintenant

Nous allons :

1. Charger un mod√®le g√©n√©ratif de type **T5** :  
   - `google/flan-t5-small` (assez l√©ger pour Colab).
2. √âcrire une fonction qui :
   - prend une **question**,
   - utilise `search(query, top_k)` pour r√©cup√©rer les meilleurs documents,
   - construit un prompt raisonnable,
   - g√©n√®re une **r√©ponse RAG**.

On obtiendra ainsi un **mini syst√®me de Question-R√©ponse bas√© sur nos documents**.

##**Chargement d'un mod√®le g√©n√©ratif (Flan-T5)**
"""

# ============================================================
# 3.1 ‚Äî Chargement d'un mod√®le g√©n√©ratif (Flan-T5)
# ============================================================

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

gen_model_name = "google/flan-t5-small"  # on peut passer √† flan-t5-base plus tard
tokenizer = AutoTokenizer.from_pretrained(gen_model_name)
gen_model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name)

gen_model = gen_model.to(device)

print(f" Mod√®le g√©n√©ratif charg√© : {gen_model_name}")

"""##**Construire le prompt RAG**"""

# ============================================================
# 3.2 ‚Äî Construction du prompt RAG
# ============================================================

def build_rag_prompt(question, retrieved_passages):
    """
    Construit un prompt texte pour le mod√®le g√©n√©ratif,
    en incluant les documents r√©cup√©r√©s.
    """
    context = ""
    for i, passage in enumerate(retrieved_passages):
        context += f"[DOC {i+1}] {passage['content']}\n"

    prompt = (
        "Tu es un assistant qui r√©pond uniquement √† partir des documents fournis.\n"
        "Si l'information n'est pas pr√©sente dans les documents, dis que tu ne sais pas.\n\n"
        "Documents :\n"
        f"{context}\n"
        "Question : "
        f"{question}\n\n"
        "R√©ponse en fran√ßais clair et p√©dagogique :"
    )
    return prompt

"""##**Fonction RAG compl√®te**"""

# ============================================================
# 3.3 ‚Äî Fonction de Question-R√©ponse RAG
# ============================================================

def rag_answer(question, top_k=3, max_new_tokens=128):
    """
    1. Retrieve des passages pertinents √† partir des documents.
    2. Construit un prompt incluant ces passages.
    3. Utilise un mod√®le g√©n√©ratif (Flan-T5) pour produire une r√©ponse.
    4. Retourne (passages_utilis√©s, r√©ponse).
    """
    # 1) R√©cup√©ration des passages
    retrieved = search(question, top_k=top_k)

    # 2) Construction du prompt
    prompt = build_rag_prompt(question, retrieved)

    # 3) G√©n√©ration
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False  # greedy pour la p√©dagogie (comportement d√©terministe)
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    return retrieved, answer

"""##**Tester le RAG sur quelques questions**"""

# ============================================================
# 3.4 ‚Äî Tests du mini syst√®me RAG
# ============================================================

test_questions = [
    "Explique ce qu'est le NLP.",
    "Comment fonctionne un mod√®le Transformer ?",
    "C'est quoi un embedding de phrase ?",
    "Qu'est-ce que le RAG en intelligence artificielle ?",
    "Comment entra√Æne-t-on un mod√®le de classification de texte ?",
    "Qu'est-ce qu'un chatbot intelligent dans ce contexte ?"
]

for q in test_questions:
    print("\n" + "="*70)
    print("‚ùì Question :", q)

    passages, answer = rag_answer(q, top_k=3)

    print("\n Passages utilis√©s :")
    for i, p in enumerate(passages):
        print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
        print(p["content"])

    print("\n R√©ponse RAG :")
    print(answer)

"""##**Analyse critique et limites**

# 3.5 ‚Äî Analyse du comportement du RAG

Nous avons maintenant un **syst√®me de Question-R√©ponse** qui :

1. Utilise les **embeddings** pour retrouver les passages les plus pertinents.
2. Construit un **prompt** avec ces passages.
3. Demande √† un **mod√®le g√©n√©ratif** (Flan-T5) de r√©pondre en se basant sur ces documents.

---

##  Points importants √† observer

1. **Les r√©ponses sont g√©n√©ralement coh√©rentes**  
   - Quand la question porte clairement sur un sujet couvert par les documents,
   - la r√©ponse reprend/condense ces informations.

2. **Le r√¥le des documents est visible**  
   - Si tu modifies la base `documents`,
   - ou que tu supprimes certains passages,
   - les r√©ponses changent.

3. **R√¥le critique du prompt**  
   - Nous avons indiqu√© au mod√®le :  
     > "R√©pond uniquement √† partir des documents fournis.  
     > Si l'information n'est pas pr√©sente, dis que tu ne sais pas."
   - Cela vise √† **r√©duire les hallucinations**, mais ne les supprime pas totalement.

---

##  Limites de cette version simplifi√©e

1. **Base documentaire minuscule**  
   - Ici, seulement quelques paragraphes.
   - Dans un vrai syst√®me RAG, on aurait des **centaines / milliers** de documents,
     d√©coup√©s en **chunks**.

2. **Pas de gestion fine du contexte**  
   - Nous collons tous les passages s√©lectionn√©s dans un seul prompt.
   - Pas de filtrage avanc√©, pas de re-ranking.

3. **Pas de m√©moire de conversation**  
   - Chaque question est trait√©e ind√©pendamment.
   - Le prochain objectif sera de construire un **mini chatbot RAG** avec historique.

4. **Mod√®le g√©n√©ratif relativement petit (Flan-T5-small)**  
   - Suffisant pour la p√©dagogie,
   - mais limit√© en "puissance" par rapport √† des mod√®les beaucoup plus grands.

---

##  √Ä retenir

- Le RAG n'est pas un "magicien" : c'est simplement **Retrieval + G√©n√©ration**.
- Toute la puissance repose sur :
  1. la qualit√© des **embeddings** (recherche s√©mantique),
  2. la qualit√© du **prompt** donn√© au mod√®le g√©n√©ratif,
  3. une bonne **conception de la base documentaire** (d√©coupage, mise √† jour‚Ä¶).

---

##  Suite : vers un mini-chatbot RAG

Prochaine √©tape possible dans ce notebook :

1. Ajouter une **boucle de dialogue** :
   - l'utilisateur pose plusieurs questions,
   - on garde un petit **historique** (les derni√®res questions/r√©ponses),
   - on continue d'utiliser `search(...)` sur les documents de base.

2. √âventuellement, montrer comment :
   - d√©couper un texte plus long en **chunks**,
   - indexer les embeddings de ces chunks,
   - faire du retrieval √† plus grande √©chelle (avec FAISS, etc.).

#**Construire un Mini-Chatbot RAG (avec Historique)**

On va :



*   Modifier notre pipeline RAG pour inclure l‚Äôhistorique de conversation.

*  Construire une boucle utilisateur simple (input ‚Üí r√©ponse).

*  G√©rer l‚Äôarr√™t de la conversation.

*  Ajouter un m√©canisme de r√©sum√© de l‚Äôhistorique si n√©cessaire (facultatif mais top).

On reste dans du Colab-friendly, donc pas de WebUI, juste du code propre.

##Intuition du Chatbot RAG

# 4. Mini Chatbot RAG ‚Äî avec Historique

Nous avons d√©j√† un pipeline :

**Question ‚Üí Retrieval ‚Üí Prompt ‚Üí G√©n√©ration**


Pour un **chatbot**, nous devons ajouter :

1. **Un historique** :  
   - m√©moriser les derni√®res questions/r√©ponses,
   - les inclure dans le prompt pour la coh√©rence.

2. **Une boucle interactive**, par exemple :

**Utilisateur ‚Üí chatbot ‚Üí r√©ponse ‚Üí utilisateur ‚Üí ...**


3. **Un m√©canisme d‚Äôarr√™t** :  
   - si l‚Äôutilisateur √©crit "stop", "exit", etc.

---

## 4.1 ‚Äî Comment g√©rer l‚Äôhistorique ?

On va simplement stocker :

**[("question_1", "r√©ponse_1"), ("question_2", "r√©ponse_2"), ...]**

et √† chaque tour, on inclut les **3 derniers √©changes** dans le prompt (windowing).  
C‚Äôest suffisant pour :

- rester **coh√©rent** dans la conversation,
- ne pas saturer le contexte du petit mod√®le T5,
- garder un code ultra p√©dagogique.

---

### Exemple de prompt (sch√©ma) :

Historique :
Utilisateur : bla bla
Assistant : r√©ponse pr√©c√©dente
Utilisateur : bla bla
Assistant : r√©ponse pr√©c√©dente

Documents pertinents :
[DOC 1] ...
[DOC 2] ...

Question actuelle : ...
R√©ponds de mani√®re claire.

C‚Äôest exactement ce que font les syst√®mes RAG industriels, mais en version simple.

---

On construit √ßa maintenant

##**Fonction pour g√©n√©rer un prompt conversationnel RAG**
"""

# ============================================================
# 4.2 ‚Äî Construction du prompt conversationnel
# ============================================================

def build_chat_prompt(history, question, retrieved_passages, max_history=3):
    """
    Construit un prompt incluant :
    - un historique (dernier max_history tours),
    - les documents pertinents,
    - la question actuelle.
    """

    # 1) Historique compact (fen√™tre glissante)
    hist_text = ""
    for (q, a) in history[-max_history:]:
        hist_text += f"Utilisateur : {q}\nAssistant : {a}\n"

    # 2) Documents retrouv√©s
    docs_text = ""
    for i, doc in enumerate(retrieved_passages):
        docs_text += f"[DOC {i+1}] {doc['content']}\n"

    # 3) Construction finale du prompt
    prompt = (
        "Tu es un assistant p√©dagogique qui r√©pond uniquement √† partir des documents fournis.\n"
        "Si l'information n'est pas dans les documents, dis que tu ne sais pas.\n\n"
        f"Historique (contexte conversationnel) :\n{hist_text}\n"
        f"Documents pertinents :\n{docs_text}\n"
        f"Question de l'utilisateur : {question}\n\n"
        "R√©ponse en fran√ßais :"
    )
    return prompt

"""##**Pipeline Chatbot-RAG complet**"""

# ============================================================
# 4.3 ‚Äî Fonction de Chatbot RAG complet
# ============================================================

def chatbot_rag_turn(history, question, top_k=3, max_new_tokens=128):
    """
    Effectue un tour de conversation :
    - retrieve
    - prompt
    - g√©n√©ration
    - mise √† jour de l'historique
    """

    # 1) Retrieval
    retrieved = search(question, top_k=top_k)

    # 2) Prompt complet (historique + docs)
    prompt = build_chat_prompt(history, question, retrieved)

    # 3) G√©n√©ration
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    # 4) Mise √† jour de l'historique
    history.append((question, answer))

    return answer, retrieved, history

"""##**Boucle interactive**"""

# ============================================================
# 4.4 ‚Äî Boucle interactive du chatbot RAG
# ============================================================

def chat():
    print("ü§ñ Chatbot RAG (tape 'quit' ou 'exit' pour arr√™ter)\n")
    history = []

    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "stop"]:
            print("Assistant : Au revoir ! üëã")
            break

        answer, retrieved, history = chatbot_rag_turn(history, user_input)

        print("\nAssistant :", answer)
        print("\nüìö Passages utilis√©s :")
        for i, p in enumerate(retrieved):
            print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
            print(p["content"])
        print("\n" + "-"*70)

# chat()  # D√©commenter pour tester en local ou dans un terminal

"""# 4.5 ‚Äî Analyse p√©dagogique du chatbot RAG

Nous avons maintenant un **chatbot capable d'utiliser des documents** pour r√©pondre.

Voici ce qu'il fait √† chaque tour :

1. **Retrieval**  
   Il utilise les embeddings pour trouver les passages les plus utiles.

2. **M√©moire courte**  
   Il prend les **3 derniers tours** pour garder un minimum de continuit√©.

3. **Prompt RAG conversationnel**  
   Il combine :
   - Historique (contexte court)
   - Documents retrouv√©s
   - Question actuelle
   ‚Üí et envoie le tout dans un mod√®le Flan-T5.

4. **G√©n√©ration contr√¥l√©e**  
   Le prompt lui demande explicitement :
   - de ne r√©pondre **QUE** √† partir des documents,
   - de dire ‚Äúje ne sais pas‚Äù si l‚Äôinfo n‚Äôest pas dans les docs.

---

## üß† Ce que les √©tudiants doivent comprendre

- Ils viennent de construire un **assistant intelligent** avec moins de 60 lignes de code.
- Le secret n'est pas un √©norme mod√®le :
  - c‚Äôest l‚Äôalliance **Retrieval + prompts contr√¥l√©s + generation**.
- Ce pipeline est **la base** de :
  - la plupart des syst√®mes RAG industriels,
  - les chatbots professionnels,
  - les assistants sp√©cialis√©s (m√©decine, industrie, juridique‚Ä¶).

---

##  √âvolution possible pour l'exercice

Une fois ma√Ætris√©, vous pouvez :

- Ajouter leurs **propres documents** (leur cours de NLP par exemple !)
- Construire un chatbot sp√©cialis√© :
  - chatbot du cours,
  - chatbot d‚Äôun syllabus,
  - chatbot de l'IATD,
  - chatbot d'un PFE,
  - chatbot de litt√©rature scientifique‚Ä¶

 Ce TP ouvre la porte aux applications r√©elles et/ou industrielles.

---

## Prochaine √©tape ?

On peut d√©velopper :

### **S5 ‚Äî D√©coupage en chunks + Indexing FAISS (RAG scalable)**  
ET  
### **S6 ‚Äî Fine-tuning de DistilBERT pour classification de sentiments**

#**Avant d'aller plus loin on va tester en direct:**

On peut tester ‚Äúen direct‚Äù dans Colab sans input():

On va faire une interface ultra simple : on lance une cellule, on change la question, on  relance.

Je propose 2 modes :

*   Mode RAG ‚Äúone-shot‚Äù (sans historique) ‚Üí pratique pour tester des questions ind√©pendantes.

*   Mode Chatbot RAG avec historique ‚Üí on garde la m√©moire entre les appels.

On r√©utilise ce qu‚Äôon a d√©j√† cod√©, on ne rajoute que des ‚Äúwrappeurs‚Äù pratiques.

##**üîπ  Mode 1 : fonction ask_rag(question) (sans historique)**
"""

# ============================================================
# 4.A ‚Äî Interface simple : RAG "one-shot" (sans historique)
# ============================================================

def ask_rag(question, top_k=3):
    """
    Pose une question au syst√®me RAG (sans historique de chat).
    Affiche directement la r√©ponse et les passages utilis√©s.
    """
    passages, answer = rag_answer(question, top_k=top_k)

    print("‚ùì Question :", question)
    print("\nüí¨ R√©ponse RAG :")
    print(answer)

    print("\nüìö Passages utilis√©s :")
    for i, p in enumerate(passages):
        print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
        print(p["content"])
    print("\n" + "-"*70)

# Exemple de test "one-shot"

ask_rag("Explique ce qu'est le RAG en intelligence artificielle.")

"""##**üîπ  Mode 2 : Chatbot RAG avec historique dans Colab**

On va cr√©er un objet simple qui garde l‚Äôhistorique en m√©moire Python et qu‚Äôon appelle comme une fonction.
"""

# ============================================================
# 4.B ‚Äî Interface Chatbot RAG avec historique (Colab-friendly)
# ============================================================

class ChatbotRAG:
    def __init__(self, top_k=3, max_new_tokens=128, max_history=3):
        self.history = []
        self.top_k = top_k
        self.max_new_tokens = max_new_tokens
        self.max_history = max_history

    def ask(self, question):
        """
        Pose une question au chatbot RAG, met √† jour l'historique
        et affiche la r√©ponse + les passages.
        """
        # 1) Retrieval
        retrieved = search(question, top_k=self.top_k)

        # 2) Prompt avec historique
        prompt = build_chat_prompt(self.history, question, retrieved, max_history=self.max_history)

        # 3) G√©n√©ration
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        output = gen_model.generate(
            **inputs,
            max_new_tokens=self.max_new_tokens,
            do_sample=False
        )
        answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # 4) Mise √† jour historique
        self.history.append((question, answer))

        # 5) Affichage
        print("‚ùì Vous :", question)
        print("\nü§ñ Assistant :", answer)

        print("\nüìö Passages utilis√©s :")
        for i, p in enumerate(retrieved):
            print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
            print(p["content"])
        print("\n" + "-"*70)

    def reset_history(self):
        self.history = []
        print("üßπ Historique du chatbot r√©initialis√©.")

"""##**Cr√©er une instance du chatbot et jouer avec**"""

# Cr√©ation du chatbot
chatbot = ChatbotRAG(top_k=3, max_new_tokens=128, max_history=3)

# Premier tour
chatbot.ask("Bonjour, peux-tu m'expliquer ce qu'est le NLP ?")

# Deuxi√®me tour, avec historique pris en compte
chatbot.ask("Et quel est le lien entre NLP et les Transformers ?")

# Troisi√®me tour
chatbot.ask("Explique ce que signifie embedding de phrase dans ce contexte.")

"""### ***N.B. Si on veut repartir de z√©ro √† tout moment  :***"""

chatbot.reset_history()

"""#**NIVEAU SUPERIEUR: Chunking + Index FAISS (RAG qui scale)**

###**Pourquoi d√©couper en chunks ?**

# 5. Du petit RAG au "vrai" RAG : d√©coupage en chunks + index vectoriel

Jusqu'ici :

- Nous avons une petite liste de documents (`documents`) contenant des paragraphes courts.
- Le retrieval se fait avec un simple produit scalaire NumPy :
  - pratique pour **quelques documents**,
  - mais pas adapt√© √† **des milliers** ou **millions** de textes.

---

## 5.1 ‚Äî Probl√®me des gros documents

Dans la r√©alit√© :

- Un cours, un rapport, un livre ‚Üí textes **tr√®s longs**.
- Un LLM (ou Flan-T5) ne peut pas lire **tout le PDF d‚Äôun coup** :
  - longueur max du *context window*,
  - co√ªt de calcul.

 **Solution** : d√©couper les textes en **chunks** (morceaux) de taille raisonnable.

Exemple :

- Un chapitre de cours ‚Üí plusieurs *chunks* de 3‚Äì5 phrases,
- Chaque chunk a son propre embedding,
- Au moment de la question :
  - on retrouve les **chunks** pertinents,
  - et on les met dans le prompt.

---

## 5.2 ‚Äî Indexer les embeddings avec FAISS

Quand il y a beaucoup de chunks, on ne peut plus :

- recalculer `np.dot` pour tous les embeddings √† chaque requ√™te,
- ou en tout cas, ce n‚Äôest plus optimal.

On va donc utiliser **FAISS** :

- une librairie d√©velopp√©e par Facebook/Meta,
- optimis√©e pour la recherche de **voisins les plus proches** dans de grands espaces vectoriels,
- tr√®s utilis√©e pour les **vector databases**.

Dans ce notebook, on va :

1. Cr√©er un long texte (mini cours),
2. Le d√©couper en chunks,
3. Calculer un embedding pour chaque chunk,
4. Construire un index FAISS,
5. Impl√©menter une fonction `search_faiss(query, top_k)`.

Nous pourrons ensuite brancher ce retrieval sur notre pipeline RAG.

## **Exemple de ‚Äúlong‚Äù document + fonction de chunking**
"""

# ============================================================
# 5.1 ‚Äî Exemple de texte long + d√©coupage en chunks
# ============================================================

long_text = """
Le traitement automatique du langage naturel, ou NLP, est une branche de l'intelligence artificielle
qui s'int√©resse √† l'interaction entre les ordinateurs et le langage humain. Historiquement, le NLP
s'appuyait sur des m√©thodes symboliques, des r√®gles linguistiques et des mod√®les statistiques simples
tels que les mod√®les n-grammes.

Avec l'essor de l'apprentissage profond, les r√©seaux de neurones r√©currents puis les Transformers
ont radicalement chang√© la mani√®re dont on traite le texte. Les mod√®les pr√©-entra√Æn√©s sur de grands
corpus, comme BERT ou GPT, permettent de capturer des repr√©sentations riches du langage.

Les embeddings de mots puis de phrases ont rendu possible la mesure de similarit√© s√©mantique entre
diff√©rents textes. Au lieu de se baser uniquement sur le vocabulaire commun, on projette les phrases
dans un espace vectoriel dense o√π la g√©om√©trie refl√®te le sens.

Les syst√®mes de question-r√©ponse modernes s'appuient souvent sur un sch√©ma de type retrieval-then-generation.
Dans un premier temps, on r√©cup√®re des passages pertinents √† partir d'une base documentaire.
Dans un second temps, un mod√®le g√©n√©ratif synth√©tise une r√©ponse coh√©rente et fluide.

Le RAG, pour Retrieval-Augmented Generation, est une architecture qui marie explicitement ces deux √©tapes.
Elle permet de connecter un mod√®le de langage √† une base de connaissances externe, mise √† jour et sp√©cifique
√† un domaine, sans devoir r√©entra√Æner compl√®tement le mod√®le.

Dans les applications industrielles, on doit souvent g√©rer des documents volumineux : manuels techniques,
rapports, proc√©dures, historiques d'incidents. Le d√©coupage en chunks et l'indexation vectorielle permettent
de faire des recherches efficaces sur ces grands corpus.
"""

# Fonction simple de d√©coupage en phrases puis en chunks de N phrases
def split_into_chunks(text, max_sentences=3):
    # 1) D√©coupage tr√®s simplifi√© en "phrases" via le point
    # (pour un vrai syst√®me, on utiliserait un vrai tokenizer de phrases)
    raw_sentences = [s.strip() for s in text.split(".") if s.strip()]

    chunks = []
    current_chunk = []

    for sent in raw_sentences:
        current_chunk.append(sent)
        if len(current_chunk) >= max_sentences:
            chunks.append(". ".join(current_chunk) + ".")
            current_chunk = []

    # Ajouter le dernier chunk s'il reste des phrases
    if current_chunk:
        chunks.append(". ".join(current_chunk) + ".")

    return chunks

chunks = split_into_chunks(long_text, max_sentences=3)

print(f"üìÑ Nombre de chunks obtenus : {len(chunks)}\n")
for i, ch in enumerate(chunks):
    print(f"--- Chunk {i} ---")
    print(ch)
    print()

"""##**Encoder les chunks et construire un index FAISS**"""

# ============================================================
# 5.2 ‚Äî Embeddings de chunks + index FAISS
# ============================================================

import faiss

# 1) Encoder les chunks
chunk_embeddings = embedder.encode(
    chunks,
    convert_to_numpy=True,
    normalize_embeddings=True
)

print("üìê Forme des embeddings de chunks :", chunk_embeddings.shape)

# 2) Dimension de l'espace d'embedding
dim = chunk_embeddings.shape[1]

# 3) Index FAISS (Inner Product, car embeddings normalis√©s ‚Üí IP = cosinus)
index = faiss.IndexFlatIP(dim)

# 4) Ajout des embeddings √† l'index
index.add(chunk_embeddings)

print("‚úÖ Index FAISS construit avec", index.ntotal, "vecteurs.")

"""##**Fonction de recherche avec FAISS**"""

# ============================================================
# 5.3 ‚Äî Recherche de chunks pertinents avec FAISS
# ============================================================

def search_chunks_faiss(query, top_k=3):
    """
    Recherche les chunks les plus pertinents pour une requ√™te donn√©e,
    en utilisant FAISS sur les embeddings de chunks.
    """
    # 1) Embedding de la requ√™te
    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)

    # 2) Recherche FAISS (scores et indices)
    scores, indices = index.search(q_emb, top_k)
    scores = scores[0]
    indices = indices[0]

    results = []
    for score, idx in zip(scores, indices):
        if idx == -1:
            continue
        results.append({
            "chunk_index": int(idx),
            "score": float(score),
            "content": chunks[idx]
        })
    return results

# Petit test
test_query = "Explique ce qu'est le RAG."
results = search_chunks_faiss(test_query, top_k=3)

print("‚ùì Requ√™te :", test_query)
print("\nüìö Chunks retourn√©s :")
for r in results:
    print(f"\n[CHUNK {r['chunk_index']}] (score={r['score']:.3f})")
    print(r["content"])

"""#**Commentaire sur FAISS + chunks**

# 5.4 ‚Äî Ce que nous avons mis en place avec FAISS

Nous avons maintenant :

1. Un **texte long** (mini cours) d√©coup√© en **chunks** de 3 phrases.
2. Un embedding pour chaque chunk.
3. Un **index FAISS** qui permet de retrouver rapidement les chunks les plus proches d'une requ√™te.

---

##  Diff√©rence avec notre premier moteur de recherche

Avant :

- Liste `documents` de quelques paragraphes,
- Retrieval par `np.dot(doc_embeddings, query_emb)`,
- Suffisant pour un petit volume.

Maintenant :

- On peut imaginer **des centaines ou milliers de chunks**,
- FAISS permet une recherche efficace, potentiellement approximative, optimis√©e C++,
- C'est le c≈ìur de ce qu'on appelle souvent une **vector database**.

---

##  Pourquoi le chunking est indispensable en RAG ?

1. **Limiter la taille des contextes envoy√©s au LLM**  
   - On ne peut pas envoyer tout un livre dans un seul prompt.
   - On envoie seulement les **chunks pertinents**.

2. **Am√©liorer la pr√©cision du retrieval**  
   - Un chunk trop long contient trop d'id√©es ‚Üí bruit.
   - Des chunks trop courts peuvent manquer de contexte.
   - Il faut **trouver un bon compromis** (ex. 2‚Äì5 phrases).

3. **√âquilibre entre granularit√© et co√ªt**  
   - Plus de chunks = plus de pr√©cision potentielle,
   - mais aussi plus de calcul d'embeddings et d'indexation.

---

##  Exercice


- Modifier `max_sentences` (2, 3, 4, 5‚Ä¶) et observer :
  - l'effet sur le nombre de chunks,
  - l'effet sur la pertinence des passages retrouv√©s.

- Remplacer `long_text` par :
  - leurs **propres notes de cours**,
  - un texte issu d'un **rapport de projet**,
  - la description d'un **PFE**.

---

##  Et ensuite ?

On peut maintenant :

-  **brancher ce retrieval FAISS** sur notre pipeline RAG :
  - en rempla√ßant la fonction `search(...)` par `search_chunks_faiss(...)`,
- On peut aussi passer √† une autre comp√©tence importante :
  - **fine-tuning d'un mod√®le de classification de sentiments** (DistilBERT).

#**RAG sur chunks : version ‚Äúone-shot‚Äù**

On va cr√©er une variante de notre fonction RAG qui utilise search_chunks_faiss au lieu de search.
"""

# ============================================================
# 5.5 ‚Äî RAG sur chunks (one-shot, sans historique)
# ============================================================

def rag_answer_chunks(question, top_k=3, max_new_tokens=128):
    """
    Version RAG qui utilise FAISS + chunks comme base documentaire.
    1. Utilise search_chunks_faiss pour r√©cup√©rer les chunks pertinents.
    2. Construit un prompt avec build_rag_prompt.
    3. G√©n√®re une r√©ponse avec le mod√®le Flan-T5.
    """
    # 1) Retrieval sur chunks
    retrieved = search_chunks_faiss(question, top_k=top_k)

    # 2) Prompt RAG (on r√©utilise exactement la m√™me fonction que pour les documents)
    prompt = build_rag_prompt(question, retrieved)

    # 3) G√©n√©ration
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    return retrieved, answer


def ask_rag_chunks(question, top_k=3):
    """
    Wrapper pratique pour tester le RAG sur chunks.
    Affiche la question, la r√©ponse, et les chunks utilis√©s.
    """
    passages, answer = rag_answer_chunks(question, top_k=top_k)

    print("‚ùì Question :", question)
    print("\nüí¨ R√©ponse RAG (sur chunks) :")
    print(answer)

    print("\nüìö Chunks utilis√©s :")
    for i, p in enumerate(passages):
        print(f"\n[CHUNK {p['chunk_index']}] (score={p['score']:.3f})")
        print(p["content"])
    print("\n" + "-"*70)

"""##**Tests RAG sur chunks**"""

# ============================================================
# 5.6 ‚Äî Tests RAG sur chunks
# ============================================================

questions_chunks = [
    "Explique ce qu'est le NLP.",
    "Comment les Transformers ont-ils chang√© le NLP ?",
    "C'est quoi un embedding de phrase ?",
    "Explique le principe du RAG.",
    "Pourquoi on d√©coupe les documents en chunks en pratique ?"
]

for q in questions_chunks:
    ask_rag_chunks(q, top_k=3)

"""#**Chatbot RAG sur chunks (avec historique)**

On va maintenant faire un chatbot RAG, mais branch√© sur FAISS + chunks.



---


On r√©utilise exactement la m√™me logique que ChatbotRAG, mais en rempla√ßant search(...) par search_chunks_faiss(...).
"""

# ============================================================
# 5.7 ‚Äî Chatbot RAG sur chunks (avec historique)
# ============================================================

class ChatbotRAGChunks:
    def __init__(self, top_k=3, max_new_tokens=128, max_history=3):
        self.history = []
        self.top_k = top_k
        self.max_new_tokens = max_new_tokens
        self.max_history = max_history

    def ask(self, question):
        """
        Pose une question au chatbot RAG (base = chunks + FAISS).
        Garde un historique court et affiche r√©ponse + chunks utilis√©s.
        """
        # 1) Retrieval sur chunks
        retrieved = search_chunks_faiss(question, top_k=self.top_k)

        # 2) Prompt avec historique + chunks
        prompt = build_chat_prompt(self.history, question, retrieved, max_history=self.max_history)

        # 3) G√©n√©ration
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        output = gen_model.generate(
            **inputs,
            max_new_tokens=self.max_new_tokens,
            do_sample=False
        )
        answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # 4) Mise √† jour de l'historique
        self.history.append((question, answer))

        # 5) Affichage
        print("‚ùì Vous :", question)
        print("\nü§ñ Assistant (RAG sur chunks) :", answer)

        print("\nüìö Chunks utilis√©s :")
        for i, p in enumerate(retrieved):
            print(f"\n[CHUNK {p['chunk_index']}] (score={p['score']:.3f})")
            print(p["content"])
        print("\n" + "-"*70)

    def reset_history(self):
        self.history = []
        print("üßπ Historique du chatbot (chunks) r√©initialis√©.")

"""##**Jouer avec le Chatbot RAG sur chunk**"""

# ============================================================
# 5.8 ‚Äî Exemple d'utilisation du ChatbotRAGChunks
# ============================================================

chat_chunks = ChatbotRAGChunks(top_k=3, max_new_tokens=128, max_history=3)

# Tour 1
chat_chunks.ask("Peux-tu m'expliquer ce qu'est le NLP, en te basant sur le texte ?")

# Tour 2
chat_chunks.ask("Quel est le r√¥le des Transformers dans cette √©volution ?")

# Tour 3
chat_chunks.ask("Et le RAG, o√π intervient-il dans ce pipeline ?")

# R√©initialiser l'historique si besoin
chat_chunks.reset_history()

"""##Architecture RAG compl√®te

# 5.9 ‚Äî Architecture RAG : "petit" vs "gros" sc√©nario

Nous avons maintenant **deux variantes** du m√™me pipeline RAG :

---

## Variante 1 ‚Äî RAG simple (documents courts en liste Python)

- Base : `documents = [doc_1, doc_2, ...]`
- Embeddings : `doc_embeddings`
- Retrieval : `search(query, top_k)` avec `np.dot`
- RAG :
  - `rag_answer(question, ...)`
  - `ChatbotRAG.ask(question)`

 Avantages :
- Parfait pour un TP *introductif*,
- Code tr√®s court,
- Facile √† comprendre.

---

## Variante 2 ‚Äî RAG scalable (texte long ‚Üí chunks + FAISS)

- Base : un ou plusieurs **textes longs** (cours, rapport, etc.)
- D√©coupage : `split_into_chunks(long_text, max_sentences=3)`
- Embeddings : `chunk_embeddings`
- Index : `faiss.IndexFlatIP(dim)` + `index.add(chunk_embeddings)`
- Retrieval : `search_chunks_faiss(query, top_k)`
- RAG :
  - `rag_answer_chunks(question, ...)`
  - `ChatbotRAGChunks.ask(question)`

 Avantages :
- Approche proche des **syst√®mes industriels**,
- G√®re des documents beaucoup plus longs,
- S√©pare clairement :
  - **pr√©traitement/indexation** (offline),
  - **requ√™tes** (online).

---

##  Message cl√©

1. **Le c≈ìur de RAG est conceptuellement simple** :
   - Encoder les textes,
   - Construire un index (m√™me simple),
   - Chercher les passages pertinents,
   - Donner ces passages au mod√®le g√©n√©ratif.

2. La diff√©rence entre un "jouet" et un "vrai syst√®me" tient surtout √† :
   - la **taille** de la base documentaire,
   - la qualit√© du **d√©coupage en chunks**,
   - le choix et la configuration de l‚Äô**index vectoriel**,
   - la **conception des prompts**.

3. Ce qu'ils voient ici est d√©j√† une **base r√©aliste** :
   - Chunking + FAISS,
   - RAG avec mod√®le open-source,
   - Conversation multi-tours avec historique.

---

√Ä ce stade, ton notebook couvre d√©j√† un **mini pipeline complet de chatbot RAG** :
- embeddings,
- retrieval simple,
- retrieval + FAISS,
- RAG,
- chatbot RAG.

La suite logique sera maintenant :

#  S6 ‚Äî Transformers & Fine-tuning DistilBERT pour la classification de sentiments


- rappel rapide BERT/DistilBERT,  
- chargement de `distilbert-base-uncased`,  
- dataset de sentiments (IMDB/SST2),  
- fine-tuning avec `Trainer`,  
- √©valuation et interpr√©tation (matrice de confusion, accuracy, F1).

#**Fine-tuning de DistilBERT pour la classification de sentiments**

# 6. Fine-tuning de DistilBERT pour la classification de sentiments

On a d√©j√† vu :

- comment faire de la **classification de sentiments** avec :
  - TF-IDF + r√©gression logistique / SVM,
  - √©ventuellement un petit r√©seau de neurones.
- comment utiliser des **embeddings de phrases** pour la similarit√©.

Maintenant, on va **entra√Æner (fine-tuner) un mod√®le Transformer** de bout en bout
pour pr√©dire directement le **sentiment** (positif / n√©gatif) d'un texte.

---

## 6.1 ‚Äî Id√©e cl√© du fine-tuning

On part d'un mod√®le **pr√©-entra√Æn√©** sur du texte brut (DistilBERT) :

- Il a d√©j√† appris une **repr√©sentation g√©n√©rale du langage** :
  - syntaxe, grammaire, relations s√©mantiques...
- On ajoute au-dessus une petite **couche de classification** (une t√™te lin√©aire).
- On r√©entra√Æne tout (ou presque tout) sur un **dataset sp√©cifique** :
  - ici : des critiques de films (positif / n√©gatif).

R√©sultat :  
Le mod√®le devient **sp√©cialis√©** pour notre t√¢che de classification de sentiments,
tout en r√©utilisant ses connaissances g√©n√©rales sur le langage.

---

## 6.2 ‚Äî Plan de cette section

1. Charger un **dataset de sentiments** (IMDB, critiques de films).
2. Pr√©parer les donn√©es :
   - texte brut ‚Üí tokens ‚Üí ids,
   - labels (0 = n√©gatif, 1 = positif).
3. Charger un mod√®le **DistilBERT** pour la classification :
   - `distilbert-base-uncased`.
4. D√©finir les param√®tres d'entra√Ænement (Trainer).
5. Lancer le **fine-tuning** sur un sous-ensemble (pour que ce soit rapide en TP).
6. √âvaluer :
   - accuracy,
   - √©ventuellement F1 / matrice de confusion.
7. Tester sur quelques phrases nouvelles.

---

Nous allons volontairement :

- **sous-√©chantillonner** le dataset (quelques milliers d'exemples),
- limiter le nombre d'√©poques,
- pour que ce soit **ex√©cutable en TP sur Colab**.

###**Charger le dataset IMDB et sous-√©chantillonner**
"""

# ============================================================
# 6.3 ‚Äî Chargement du dataset IMDB (sentiment)
# ============================================================

from datasets import load_dataset

# Dataset IMDB : critiques de films (texte + label 0/1)
imdb = load_dataset("imdb")

print(imdb)

# Par d√©faut :
# - train : 25 000 exemples
# - test  : 25 000 exemples

# Pour que ce soit plus rapide en TP, on prend un sous-ensemble
small_train_size = 2000
small_test_size = 1000

small_train_dataset = imdb["train"].shuffle(seed=42).select(range(small_train_size))
small_test_dataset = imdb["test"].shuffle(seed=42).select(range(small_test_size))

print("\nüìä Tailles apr√®s sous-√©chantillonnage :")
print("Train :", len(small_train_dataset))
print("Test  :", len(small_test_dataset))

"""##***Aper√ßu de quelques exemples du dataset***

**NB :** label = 0 ‚Üí g√©n√©ralement n√©gatif, 1 ‚Üí positif (IMDB).
"""

# ============================================================
# 6.4 ‚Äî Aper√ßu de quelques exemples du dataset
# ============================================================

for i in range(3):
    print(f"--- Exemple {i} ---")
    print("Texte :", small_train_dataset[i]["text"][:300], "...")
    print("Label :", small_train_dataset[i]["label"])
    print()

"""##**Tokenizer DistilBERT et pr√©paration des donn√©es**"""

# ============================================================
# 6.5 ‚Äî Tokenizer DistilBERT et pr√©paration des donn√©es
# ============================================================

from transformers import AutoTokenizer

model_ckpt = "distilbert-base-uncased"
tokenizer_cls = AutoTokenizer.from_pretrained(model_ckpt)

# Fonction de tokenisation appliqu√©e au dataset
def tokenize_batch(batch):
    return tokenizer_cls(
        batch["text"],
        padding="max_length",      # on peut aussi utiliser padding="longest"
        truncation=True,
        max_length=256
    )

# Application au train et au test
tokenized_train = small_train_dataset.map(tokenize_batch, batched=True)
tokenized_test = small_test_dataset.map(tokenize_batch, batched=True)

# On d√©finit les colonnes √† garder pour le Trainer
tokenized_train = tokenized_train.remove_columns(["text"])
tokenized_test = tokenized_test.remove_columns(["text"])

tokenized_train = tokenized_train.with_format("torch")
tokenized_test = tokenized_test.with_format("torch")

print(tokenized_train)

"""##**Mod√®le DistilBERT pour classification de s√©quence**"""

# ============================================================
# 6.6 ‚Äî Mod√®le DistilBERT pour classification de s√©quence
# ============================================================

from transformers import AutoModelForSequenceClassification

num_labels = 2  # IMDB : n√©gatif / positif

model_cls = AutoModelForSequenceClassification.from_pretrained(
    model_ckpt,
    num_labels=num_labels
).to(device)

print("‚úÖ Mod√®le de classification charg√© :", model_ckpt)

# ============================================================
# 6.7 ‚Äî Configuration de l'entra√Ænement (Trainer)
# Compatible Transformers ‚â• 4.39
# ============================================================

!pip install -q evaluate

from transformers import TrainingArguments, Trainer
from transformers import DataCollatorWithPadding
import numpy as np
from evaluate import load as load_metric

# Data collator pour g√©rer le padding dynamique
data_collator = DataCollatorWithPadding(tokenizer_cls)

# M√©trique : accuracy
metric_acc = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric_acc.compute(predictions=preds, references=labels)

batch_size = 16

#  La bonne signature pour la version de Transformers :
# ‚Üí eval_strategy (PAS evaluation_strategy)
training_args = TrainingArguments(
    output_dir="./distilbert-sentiment-imdb",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

trainer = Trainer(
    model=model_cls,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer_cls,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

print("‚úÖ Trainer configur√© ‚Äî version 100% compatible avec la version HF.")

"""##Fine-tuning de DistilBERT (entra√Ænement)"""

# ============================================================
# 6.8 ‚Äî Fine-tuning de DistilBERT (entra√Ænement)
# ============================================================

train_result = trainer.train()
trainer.save_model()

print("\nüèÅ Entra√Ænement termin√©.")

# ============================================================
# 6.9 ‚Äî √âvaluation sur le jeu de test
# ============================================================

eval_result = trainer.evaluate()
print("\nüìä R√©sultats d'√©valuation :")
for k, v in eval_result.items():
    print(f"{k}: {v}")

# ============================================================
# 6.10 ‚Äî Rapport de classification + matrice de confusion
# ============================================================

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import torch

# R√©cup√©rer les pr√©dictions finales sur le test set
preds_output = trainer.predict(tokenized_test)
logits = preds_output.predictions
y_pred = np.argmax(logits, axis=-1)
y_true = preds_output.label_ids

print("üîé Rapport de classification :\n")
print(classification_report(y_true, y_pred, digits=3))

print("üß© Matrice de confusion :\n")
print(confusion_matrix(y_true, y_pred))

# ============================================================
# 6.11 ‚Äî Fonction pratique : pr√©dire le sentiment d'un texte
# ============================================================

label_names = {0: "n√©gatif", 1: "positif"}

def predict_sentiment(text):
    model_cls.eval()
    inputs = tokenizer_cls(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    ).to(device)

    with torch.no_grad():
        outputs = model_cls(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()
        pred = int(np.argmax(probs))

    print("Texte :", text)
    print(f"‚Üí Sentiment pr√©dit : {label_names[pred]} (proba={probs[pred]:.3f})\n")

# Exemple de test rapide
predict_sentiment("This movie was absolutely fantastic, I loved it.")
predict_sentiment("The plot was boring and the acting was terrible.")

# ============================================================
# 7.1 ‚Äî Courbes d'apprentissage : loss & accuracy
# ============================================================

import matplotlib.pyplot as plt

# trainer.state.log_history contient l'historique des logs
logs = trainer.state.log_history

train_steps = []
train_loss = []
eval_steps = []
eval_loss = []
eval_accuracy = []

for entry in logs:
    # Logs d'entra√Ænement (loss)
    if "loss" in entry and "epoch" in entry and "step" in entry:
        train_steps.append(entry["step"])
        train_loss.append(entry["loss"])
    # Logs d'√©valuation
    if "eval_loss" in entry:
        eval_steps.append(entry["step"])
        eval_loss.append(entry["eval_loss"])
    if "eval_accuracy" in entry:
        eval_accuracy.append(entry["eval_accuracy"])

plt.figure(figsize=(8, 4))
plt.plot(train_steps, train_loss, label="Train loss")
plt.plot(eval_steps, eval_loss, label="Eval loss")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("√âvolution de la loss (train vs eval)")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 4))
plt.plot(eval_steps, eval_accuracy, marker="o")
plt.xlabel("Step")
plt.ylabel("Accuracy (eval)")
plt.title("√âvolution de l'accuracy sur le jeu de validation")
plt.grid(True)
plt.show()

"""## 7.2 ‚Äî Lecture des courbes d'apprentissage

En regardant les graphiques :

- La **loss d'entra√Ænement** doit en g√©n√©ral **d√©cro√Ætre** au fil des steps.
- La **loss de validation** :
  - d√©cro√Æt aussi si le mod√®le apprend,
  - peut remonter si on commence √† **sur-apprendre** (overfitting).

L'**accuracy de validation** permet de voir :

- si le mod√®le **progresse** au fil des epochs,
- si elle se stabilise,
- ou si elle se d√©grade (signe potentiel d'overfitting ou de probl√®mes de g√©n√©ralisation).

 On peut se donner comme t√¢ches :
- de comparer ces courbes avec celles d‚Äôun **mod√®le classique** (TF-IDF + r√©gression logistique),
- d‚Äôajuster le **nombre d‚Äôepochs** (1, 2, 3...) et d‚Äôobserver l‚Äôimpact sur les courbes.

---


#**‚úÖDe la QA au chatbot de discussion‚úÖ**


---

# 10. Transformer le RAG en chatbot de discussion

Jusqu'ici, notre RAG r√©pond √† des **questions ind√©pendantes** :

- une question ‚Üí retrieval ‚Üí g√©n√©ration ‚Üí r√©ponse.

Pour obtenir un **chatbot de discussion**, il faut simplement ajouter :

1. Une **m√©moire de conversation** (historique des tours pr√©c√©dents) :
   - garder les derni√®res questions/r√©ponses,
   - les injecter dans le prompt pour conserver le fil.

2. Une **persona** / r√¥le :
   - par exemple : "assistant p√©dagogique", "tuteur de NLP", etc.
   - cela se fait principalement dans le **prompt** (aucun code magique suppl√©mentaire).

3. Une **interface d‚Äôappel simple** :
   - une m√©thode `ask(...)` qui :
     - re√ßoit le message utilisateur,
     - met √† jour l‚Äôhistorique,
     - appelle le RAG,
     - renvoie la r√©ponse.

---

## 10.1 ‚Äî Architecture conceptuelle du chatbot RAG

Sch√©ma simplifi√© d‚Äôun tour de dialogue :

1. **Entr√©e utilisateur** : `message_t`
2. **Construction du contexte** :
   - on prend les `k` derniers (question, r√©ponse),
   - on ajoute le `message_t`.
3. **Retrieval** (sur chunks) :
   - on encode `message_t`,
   - on cherche les chunks les plus proches dans l‚Äôindex FAISS.
4. **Prompt RAG conversationnel** :
   - r√¥le + consignes,
   - historique,
   - documents pertinents,
   - question actuelle.
5. **Mod√®le g√©n√©ratif** (Flan-T5) :
   - produit une r√©ponse `r√©ponse_t`.
6. **Mise √† jour de l‚Äôhistorique** :
   - on ajoute `(message_t, r√©ponse_t)`.

 L‚Äôid√©e cl√© : *"Un chatbot RAG n‚Äôest rien d‚Äôautre qu‚Äôun RAG + un peu de m√©moire."*

##**Chatbot RAG ‚Äúpropre‚Äù bas√© chunks**

On part de search_chunks_faiss et build_chat_prompt qu‚Äôon avait d√©j√†.
On va juste simplifier et ‚Äúofficialiser‚Äù une classe ConversationalRAG.
"""

# ============================================================
# 10.2 ‚Äî Chatbot RAG de discussion (sans sentiment)
# Base : RAG sur chunks + historique
# ============================================================

class ConversationalRAG:
    def __init__(self, top_k=3, max_new_tokens=128, max_history=3):
        """
        top_k        : nombre de chunks √† r√©cup√©rer √† chaque tour
        max_new_tokens : longueur max de la r√©ponse g√©n√©r√©e
        max_history  : nombre de tours pr√©c√©dents √† inclure dans le prompt
        """
        self.history = []  # liste de tuples (question, answer)
        self.top_k = top_k
        self.max_new_tokens = max_new_tokens
        self.max_history = max_history

    def build_prompt(self, user_message, retrieved_passages):
        """
        Construit un prompt conversationnel pour Flan-T5,
        en utilisant l'historique et les chunks pertinents.
        """
        # Historique compact
        hist_text = ""
        for (q, a) in self.history[-self.max_history:]:
            hist_text += f"Utilisateur : {q}\nAssistant : {a}\n"

        # Documents
        docs_text = ""
        for i, doc in enumerate(retrieved_passages):
            docs_text += f"[DOC {i+1}] {doc['content']}\n"

        prompt = (
            "Tu es un assistant p√©dagogique sp√©cialis√© dans ce texte.\n"
            "Tu dois r√©pondre UNIQUEMENT √† partir des documents fournis.\n"
            "Si une information n'est pas dans les documents, dis honn√™tement que tu ne sais pas.\n\n"
            "Historique r√©cent de la conversation :\n"
            f"{hist_text}\n"
            "Documents pertinents :\n"
            f"{docs_text}\n"
            f"Message actuel de l'utilisateur : {user_message}\n\n"
            "R√©ponse en fran√ßais, claire et structur√©e :"
        )
        return prompt

    def ask(self, user_message):
        """
        Un tour de conversation :
        - retrieval sur les chunks,
        - g√©n√©ration de la r√©ponse,
        - mise √† jour de l'historique,
        - affichage p√©dagogique.
        """
        # 1) Retrieval
        retrieved = search_chunks_faiss(user_message, top_k=self.top_k)

        # 2) Prompt
        prompt = self.build_prompt(user_message, retrieved)

        # 3) G√©n√©ration
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        output = gen_model.generate(
            **inputs,
            max_new_tokens=self.max_new_tokens,
            do_sample=False
        )
        answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # 4) Historique
        self.history.append((user_message, answer))

        # 5) Affichage
        print("‚ùì Vous :", user_message)
        print("\nü§ñ Assistant :")
        print(answer)

        print("\nüìö Chunks utilis√©s :")
        for i, p in enumerate(retrieved):
            print(f"\n[CHUNK {p['chunk_index']}] (score={p['score']:.3f})")
            print(p["content"])
        print("\n" + "-"*80)

    def reset_history(self):
        self.history = []
        print("üßπ Historique du chatbot RAG r√©initialis√©.")

"""##**Exemple de session de discussion**"""

# ============================================================
# 10.3 ‚Äî Exemple de session de discussion
# ============================================================

chat_discuss = ConversationalRAG(top_k=3, max_new_tokens=128, max_history=3)

# Tour 1 : question de base
chat_discuss.ask("Peux-tu m'expliquer ce qu'est le NLP selon ce texte ?")

# Tour 2 : question de suivi
chat_discuss.ask("Et en quoi les Transformers ont chang√© la mani√®re de faire du NLP ?")

# Tour 3 : question sur le RAG
chat_discuss.ask("Explique comment le RAG s'ins√®re dans ce pipeline moderne.")

# Pour repartir √† z√©ro :
# chat_discuss.reset_history()

"""## 10.4 ‚Äî Pourquoi ce chatbot est d√©j√† un "vrai" assistant de discussion ?

Ce chatbot :

1. **R√©utilise le RAG sur chunks** :
   - chaque message d√©clenche un retrieval sur l'index FAISS,
   - les r√©ponses sont ancr√©es dans le texte fourni.

2. **G√®re un historique simple** (fen√™tre glissante) :
   - permet d'encha√Æner des questions,
   - le mod√®le voit un contexte de conversation.

3. **Est contr√¥l√© par le prompt** :
   - persona (assistant p√©dagogique),
   - consigne de ne pas inventer,
   - style (clair, structur√©).

---

### üß† D√©mystification

Il n'y a pas de magie cach√©e :

- Pas besoin d'un "moteur de dialogue" suppl√©mentaire,
- Pas besoin d'un algorithme compliqu√© de gestion de contexte.

Un **chatbot RAG moderne**, dans beaucoup d'architectures r√©elles, c‚Äôest simplement :

> üîπ des embeddings +  
> üîπ un index vectoriel +  
> üîπ un peu de logique Python (historique) +  
> üîπ un mod√®le g√©n√©ratif +  
> üîπ un prompt bien con√ßu.

Tout ce que nous avons fait dans ce notebook.

---

Pour aller plus loin, on peut envisager un **petit fine-tuning de style** du mod√®le g√©n√©ratif, ce qui est l‚Äôobjet de la section suivante.

# 11. Mini fine-tuning de style (id√©e avanc√©e)

Jusqu'ici, nous avons contr√¥l√© le comportement du mod√®le g√©n√©ratif principalement par :

- le **prompt** (persona, consignes),
- le contexte (historique + documents).

On peut aller plus loin avec un **fine-tuning l√©ger** :

- prendre un petit mod√®le (ex. `google/flan-t5-small`),
- pr√©parer quelques dizaines / centaines d'exemples de type :

  - *Entr√©e* : "Question de l'√©tudiant + extraits du cours"
  - *Sortie attendue* : "R√©ponse dans le style que l'on souhaite"  
    (par exemple : tr√®s p√©dagogique, avec plan, exemples, etc.)

- faire un fine-tuning **court** sur ces paires (instruction-tuning "local").

L'objectif n'est pas d'am√©liorer radicalement la performance,
mais d'**adapter le ton et le style** de l'assistant :

- plus structur√©,
- plus acad√©mique,
- plus adapt√© √† ton contexte de cours.

Pour des raisons de temps et de ressources, on se contente ici d'un **squelette** de code.
"""

# ============================================================
# 11.2 ‚Äî Squelette de mini fine-tuning de style (OPTIONNEL)
# ============================================================
# ‚ö†Ô∏è √Ä n'ex√©cuter que si tu veux exp√©rimenter en profondeur.
# Id√©e : cr√©er un petit dataset "input ‚Üí output" avec le style souhait√©.

from datasets import Dataset
from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Exemples jouets : √† remplacer par tes propres paires (cours, explications...)
examples = [
    {
        "input": "Explique ce qu'est le NLP √† un √©tudiant d√©butant.",
        "target": "Le traitement automatique du langage naturel (NLP) est une branche de l'IA qui vise √† permettre aux machines de comprendre et de g√©n√©rer du langage humain. Par exemple, la classification de sentiments, la traduction automatique, ou les chatbots."
    },
    {
        "input": "Explique en quelques phrases le principe du RAG.",
        "target": "Le RAG, pour Retrieval-Augmented Generation, combine deux √©tapes : d'abord, on r√©cup√®re des passages pertinents dans une base de documents ; ensuite, un mod√®le de g√©n√©ration produit une r√©ponse en s'appuyant sur ces passages. Cela permet d'ancrer les r√©ponses dans une connaissance externe mise √† jour."
    },
    {
        "input": "Explique simplement ce qu'est un embedding de phrase.",
        "target": "Un embedding de phrase est une repr√©sentation num√©rique compacte d'une phrase sous forme de vecteur. Deux phrases qui veulent dire √† peu pr√®s la m√™me chose auront des vecteurs proches dans cet espace."
    }
]

style_dataset = Dataset.from_list(examples)

def preprocess_style_batch(batch):
    model_input = [
        "Instruction : " + inp + "\nR√©ponse p√©dagogique :"
        for inp in batch["input"]
    ]
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["target"], truncation=True, padding="max_length", max_length=128)
    encodings = tokenizer(model_input, truncation=True, padding="max_length", max_length=128)
    encodings["labels"] = labels["input_ids"]
    return encodings

tokenizer = tokenizer  # alias pour clart√©
style_dataset_tokenized = style_dataset.map(preprocess_style_batch, batched=True)

data_collator_seq2seq = DataCollatorForSeq2Seq(tokenizer, model=gen_model)

ft_args = Seq2SeqTrainingArguments(
    output_dir="./flan-t5-style-ft",
    per_device_train_batch_size=2,
    learning_rate=5e-5,
    num_train_epochs=3,
    logging_steps=1,
    save_strategy="no",
    predict_with_generate=True
)

style_trainer = Seq2SeqTrainer(
    model=gen_model,
    args=ft_args,
    train_dataset=style_dataset_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator_seq2seq
)

#  √Ä lancer seulement si tu as le temps / GPU dispo
# style_trainer.train()

#  √Ä lancer seulement si tu as le temps / GPU dispo
style_trainer.train()

"""## 11.3 ‚Äî Conclusion sur le fine-tuning de style

Ce squelette illustre l'id√©e suivante :

- On peut **prendre le m√™me mod√®le g√©n√©ratif** (Flan-T5),
- le **r√©entra√Æner l√©g√®rement** sur quelques exemples bien choisis,
- pour "caler" son style sur :
  - celui d'un enseignant,
  - d'un support de cours,
  - d'une √©cole ou d'une entreprise.



---

Avec ce bloc, ce notebook offre une vue compl√®te :

- RAG ‚Üí QA,
- RAG ‚Üí Chatbot de discussion,
- et une porte d'entr√©e vers **l'adaptation de style** par fine-tuning.

---



---

---



---



---



---

‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ
---



---



---

# 11. Mini fine-tuning de style (BONUS, option avanc√©e)

Dans tout le notebook, nous avons contr√¥l√© le comportement du mod√®le g√©n√©ratif
principalement par :

- le **prompt** (persona, consignes),
- le **contexte** (historique + documents).

Cette section montre, de fa√ßon **purement illustrative**, comment faire un
**mini fine-tuning de style** sur le mod√®le g√©n√©ratif (Flan-T5).

üéØ **Objectif p√©dagogique** :

- Comprendre la **pipeline g√©n√©rale** d‚Äôun fine-tuning Seq2Seq :
  - pr√©paration d‚Äôun petit dataset (input ‚Üí output),
  - tokenisation,
  - entra√Ænement avec `Seq2SeqTrainer`.
- Ne PAS chercher ici :
  - des performances √©lev√©es,
  - une loss "belle" ou parfaitement stable.

‚ö†Ô∏è **Consignes ** :

- Cette section est **optionnelle** et peut √™tre ignor√©e sans probl√®me.
- Ne lancez le fine-tuning que si :
  - vous √™tes sur GPU,
  - vous avez le temps (quelques minutes).
- Le dataset est volontairement **tr√®s petit (‚âà 10‚Äì15 exemples)** :
  - ce n‚Äôest pas pour battre des records,
  - mais pour illustrer la m√©canique.
"""

# ============================================================
# 11.2 ‚Äî Mini dataset de style : questions ‚Üí r√©ponses p√©dagogiques
# ============================================================

from datasets import Dataset

style_examples = [
    {
        "input": "Explique ce qu'est le NLP √† un √©tudiant d√©butant.",
        "target": ("Le traitement automatique du langage naturel (NLP) est une branche de "
                   "l'intelligence artificielle qui vise √† permettre aux machines de comprendre "
                   "et de g√©n√©rer du langage humain. Par exemple, on l'utilise pour les "
                   "chatbots, la classification de sentiments ou la traduction automatique.")
    },
    {
        "input": "Explique simplement ce qu'est un embedding de phrase.",
        "target": ("Un embedding de phrase est une fa√ßon de repr√©senter une phrase sous forme "
                   "d'un vecteur de nombres. Deux phrases qui ont un sens proche auront des "
                   "vecteurs proches, ce qui permet de mesurer leur similarit√©.")
    },
    {
        "input": "Explique en quelques phrases le principe du RAG.",
        "target": ("Le RAG, pour Retrieval-Augmented Generation, combine deux √©tapes : d'abord, "
                   "on r√©cup√®re des passages pertinents dans une base de documents ; ensuite, "
                   "un mod√®le de g√©n√©ration produit une r√©ponse en s'appuyant sur ces passages. "
                   "Cela permet d'ancrer les r√©ponses dans une connaissance externe mise √† jour.")
    },
    {
        "input": "Explique le lien entre embeddings et moteur de recherche s√©mantique.",
        "target": ("Un moteur de recherche s√©mantique utilise des embeddings pour repr√©senter "
                   "les phrases et les documents. Quand l'utilisateur pose une question, on la "
                   "convertit en vecteur et on cherche les documents dont les vecteurs sont les "
                   "plus proches. On r√©cup√®re ainsi les textes les plus proches en sens.")
    },
    {
        "input": "Explique la diff√©rence entre TF-IDF et embeddings modernes.",
        "target": ("TF-IDF repr√©sente chaque document par des compteurs de mots pond√©r√©s, sans "
                   "vraiment comprendre le sens. Les embeddings modernes, eux, produisent des "
                   "vecteurs denses qui capturent la signification globale du texte et les "
                   "relations entre les mots.")
    },
    {
        "input": "Explique ce qu'apporte un Transformer par rapport √† un RNN.",
        "target": ("Les Transformers utilisent un m√©canisme d'attention qui permet de regarder "
                   "tous les mots de la phrase en parall√®le et de mod√©liser plus facilement les "
                   "d√©pendances longues. Ils sont plus simples √† entra√Æner que les RNN profonds "
                   "et sont devenus la base de la plupart des mod√®les de NLP modernes.")
    },
    {
        "input": "Explique pourquoi on d√©coupe un long document en chunks pour le RAG.",
        "target": ("On d√©coupe un long document en chunks pour que chaque morceau ait une taille "
                   "g√©rable par le mod√®le et pour pouvoir indexer finement le contenu. Au moment "
                   "d'une question, on ne charge que quelques chunks pertinents au lieu de tout "
                   "le document, ce qui est plus efficace et plus pr√©cis.")
    },
    {
        "input": "Explique la notion de base de connaissances externe dans le RAG.",
        "target": ("Dans un syst√®me RAG, la base de connaissances externe contient les documents "
                   "sur lesquels on veut s'appuyer : cours, articles, documentation technique, "
                   "etc. Le mod√®le n'a pas besoin de tout m√©moriser dans ses poids, il consulte "
                   "cette base √† la demande via le retrieval.")
    },
    {
        "input": "Explique comment un chatbot RAG utilise l'historique de la conversation.",
        "target": ("Un chatbot RAG garde en m√©moire les derniers √©changes sous forme de texte. "
                   "Lors d'une nouvelle question, il inclut cet historique dans le prompt, en "
                   "plus des documents r√©cup√©r√©s. Cela lui permet de r√©pondre de mani√®re coh√©rente "
                   "avec le fil de la discussion.")
    },
    {
        "input": "Explique en une phrase l'id√©e g√©n√©rale de ce notebook.",
        "target": ("Ce notebook montre comment passer des m√©thodes de NLP classiques √† un "
                   "pipeline moderne combinant embeddings, retrieval, RAG et Transformers pour "
                   "la classification et la g√©n√©ration.")
    },
    {
        "input": "Donne une courte d√©finition p√©dagogique de Transformer.",
        "target": ("Un Transformer est un type de r√©seau de neurones qui utilise l'attention "
                   "pour traiter des s√©quences en parall√®le et mod√©liser les relations entre "
                   "les √©l√©ments d'une phrase ou d'un texte.")
    },
    {
        "input": "Donne une courte d√©finition p√©dagogique de chatbot RAG.",
        "target": ("Un chatbot RAG est un assistant conversationnel qui va d'abord chercher des "
                   "documents pertinents, puis g√©n√®re une r√©ponse en se basant sur ces documents, "
                   "plut√¥t que sur sa seule m√©moire interne.")
    }
]

style_dataset = Dataset.from_list(style_examples)
style_dataset

# ============================================================
# 11.3 ‚Äî Pr√©traitement du mini dataset pour Flan-T5
# ============================================================

from transformers import DataCollatorForSeq2Seq

# On utilise le tokenizer de Flan-T5 (d√©j√† d√©fini plus haut : `tokenizer`)
max_input_length = 128
max_target_length = 128

def preprocess_style_batch(batch):
    # On construit un texte d'entr√©e l√©g√®rement "instructionnalis√©"
    inputs = [
        "Instruction : " + inp + "\nR√©ponse attendue :"
        for inp in batch["input"]
    ]

    model_inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length"
    )

    # Labels (texte cible)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            batch["target"],
            max_length=max_target_length,
            truncation=True,
            padding="max_length"
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

style_dataset_tokenized = style_dataset.map(preprocess_style_batch, batched=True)

data_collator_seq2seq = DataCollatorForSeq2Seq(tokenizer, model=gen_model)

style_dataset_tokenized

# ============================================================
# 11.4 ‚Äî Configuration du mini fine-tuning (sans W&B)
# ============================================================

import os
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

# D√©sactiver totalement Weights & Biases pour ce notebook
os.environ["WANDB_DISABLED"] = "true"

ft_args = Seq2SeqTrainingArguments(
    output_dir="./flan-t5-style-ft",
    per_device_train_batch_size=2,
    learning_rate=5e-5,
    num_train_epochs=3,
    logging_steps=1,
    save_strategy="no",       # on ne sauvegarde pas √† chaque epoch
    predict_with_generate=True,
    report_to="none"          # ‚úÖ pas de wandb, pas de tensorboard
)

style_trainer = Seq2SeqTrainer(
    model=gen_model,
    args=ft_args,
    train_dataset=style_dataset_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator_seq2seq,
)

print("‚úÖ Trainer style configur√© (mini dataset, pas de W&B).")

# ============================================================
# 11.5 ‚Äî Lancement OPTIONNEL du mini fine-tuning
# ============================================================



style_trainer.train()

# ============================================================
# 11.6 ‚Äî Test qualitatif apr√®s (√©ventuel) fine-tuning
# ============================================================

def generate_pedago_answer(instruction, max_new_tokens=128):
    """
    Petit helper pour interroger Flan-T5 dans le format de la t√¢che de style.
    """
    prompt = f"Instruction : {instruction}\nR√©ponse attendue :"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    print("Instruction :", instruction)
    print("\nR√©ponse du mod√®le :")
    print(answer)
    print("\n" + "-"*60)

# Exemple de tests (avant et/ou apr√®s fine-tuning)
generate_pedago_answer("Explique ce qu'est le NLP √† un √©tudiant d√©butant.")
generate_pedago_answer("Explique en quelques phrases le principe du RAG.")
generate_pedago_answer("Explique simplement ce qu'est un embedding de phrase.")

documents = [
    """La cybers√©curit√© regroupe l‚Äôensemble des techniques et
    outils destin√©s √† prot√©ger les syst√®mes informatiques
    contre les attaques et les acc√®s non autoris√©s.""",

    """Les pare-feux modernes utilisent des r√®gles dynamiques
    et des mod√®les comportementaux pour bloquer le trafic
    suspect en temps r√©el.""",

    """La cryptographie assure la confidentialit√© et
    l‚Äôint√©grit√© des donn√©es gr√¢ce √† des algorithmes de
    chiffrement et de signature.""",

    """Pour d√©tecter une intrusion, on collecte des journaux
    d‚Äô√©v√©nements, puis on entra√Æne un mod√®le qui rep√®re les
    anomalies dans ces donn√©es.""",

    """Les empreintes num√©riques permettent d‚Äôidentifier un
    fichier ou un message gr√¢ce √† un condensat unique cr√©√©
    par une fonction de hachage.""",

    """Le Zero Trust repose sur l‚Äôid√©e qu‚Äôaucune entit√© ne
    doit √™tre automatiquement consid√©r√©e comme fiable, m√™me
    √† l‚Äôint√©rieur du r√©seau.""",

    """Un syst√®me de d√©tection avanc√©e peut combiner analyse
    comportementale et intelligence artificielle pour
    identifier des menaces nouvelles ou √©volutives."""
]