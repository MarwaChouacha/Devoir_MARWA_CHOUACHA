# -*- coding: utf-8 -*-
"""Livrable6_MARWA_CHOUACHA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfGmHRglIzZWi-npV4wF0l8A6byIoKb8
"""

# ============================================================
# 0.2 ‚Äî Installation des biblioth√®ques n√©cessaires
# √Ä ex√©cuter une seule fois au d√©but du notebook (Colab)
# ============================================================

!pip install -q transformers datasets sentence-transformers accelerate faiss-cpu

import torch
import transformers
import datasets
from sentence_transformers import SentenceTransformer

print(" PyTorch version :", torch.__version__)
print(" Transformers version :", transformers.__version__)
print(" Datasets version :", datasets.__version__)

# V√©rif rapide du GPU (optionnel, mais motivant pour les √©tudiants)
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(" GPU disponible :", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print(" Pas de GPU d√©tect√©, on reste sur CPU.")

# ============================================================
# 1.3 ‚Äî Premi√®re exp√©rience : embeddings & similarit√© cosinus
# ============================================================

from sentence_transformers import SentenceTransformer
import numpy as np

# 1) Charger le mod√®le de sentence embeddings
model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedder = SentenceTransformer(model_name)
print(f" Mod√®le charg√© : {model_name}")

# 2) D√©finir quelques phrases de test (en fran√ßais)
sentences = [
    "Ce film √©tait incroyable, j'ai ador√© chaque minute.",
    "J'ai vraiment aim√© ce film, il √©tait excellent.",
    "Ce film est une perte de temps, je le d√©teste.",
    "Il fait tr√®s beau aujourd'hui, le soleil brille.",
    "Le temps est magnifique, il y a beaucoup de soleil.",
    "Je dois acheter du pain √† la boulangerie."
]

print("\n Phrases test :")
for i, s in enumerate(sentences):
    print(f"{i}: {s}")

# 3) Calculer les embeddings (matrice [nb_phrases, dim_embedding])
embeddings = embedder.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)
print("\n Forme de la matrice d'embeddings :", embeddings.shape)

# 4) Fonction de similarit√© cosinus
def cosine_similarity_matrix(X):
    """
    X : matrice (n, d) o√π n = nb de phrases, d = dimension embedding
    Retourne une matrice (n, n) des similarit√©s cosinus.
    """
    # X est d√©j√† normalis√© (norme 1) => produit scalaire = cosinus
    return np.matmul(X, X.T)

sim_matrix = cosine_similarity_matrix(embeddings)

# 5) Afficher la matrice de similarit√© de mani√®re lisible
np.set_printoptions(precision=2, suppress=True)
print("\n Matrice de similarit√© cosinus entre phrases :\n")
print(sim_matrix)

# 6) Afficher quelques similarit√©s cibl√©es
def show_similarity(i, j):
    print(f"\nSim({i}, {j}) = cosinus( phrase[{i}], phrase[{j}] ) = {sim_matrix[i, j]:.3f}")
    print(f"  phrase[{i}] : {sentences[i]}")
    print(f"  phrase[{j}] : {sentences[j]}")

# Phrases tr√®s proches en sens (film positif)
show_similarity(0, 1)

# Film positif vs film n√©gatif
show_similarity(0, 2)

# Deux phrases sur la m√©t√©o
show_similarity(3, 4)

# Phrase sur la m√©t√©o vs phrase sur la boulangerie
show_similarity(3, 5)

documents = [
    """La cybers√©curit√© regroupe l‚Äôensemble des techniques et
    outils destin√©s √† prot√©ger les syst√®mes informatiques
    contre les attaques et les acc√®s non autoris√©s.""",

    """Les pare-feux modernes utilisent des r√®gles dynamiques
    et des mod√®les comportementaux pour bloquer le trafic
    suspect en temps r√©el.""",

    """La cryptographie assure la confidentialit√© et
    l‚Äôint√©grit√© des donn√©es gr√¢ce √† des algorithmes de
    chiffrement et de signature.""",

    """Pour d√©tecter une intrusion, on collecte des journaux
    d‚Äô√©v√©nements, puis on entra√Æne un mod√®le qui rep√®re les
    anomalies dans ces donn√©es.""",

    """Les empreintes num√©riques permettent d‚Äôidentifier un
    fichier ou un message gr√¢ce √† un condensat unique cr√©√©
    par une fonction de hachage.""",

    """Le Zero Trust repose sur l‚Äôid√©e qu‚Äôaucune entit√© ne
    doit √™tre automatiquement consid√©r√©e comme fiable, m√™me
    √† l‚Äôint√©rieur du r√©seau.""",

    """Un syst√®me de d√©tection avanc√©e peut combiner analyse
    comportementale et intelligence artificielle pour
    identifier des menaces nouvelles ou √©volutives."""
]

# ============================================================
# 2.2 ‚Äî Encoder les documents
# ============================================================

# On r√©utilise le mod√®le embedder charg√© dans S1
doc_embeddings = embedder.encode(
    documents,
    convert_to_numpy=True,
    normalize_embeddings=True
)

print("Embeddings documents :", doc_embeddings.shape)

# ============================================================
# 2.3 ‚Äî Fonction de recherche s√©mantique
# ============================================================

def search(query, top_k=3):
    """
    Retourne les top_k documents les plus proches de la requ√™te.
    """
    # Encoder la requ√™te
    query_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]

    # Calcul des similarit√©s cosinus (produit scalaire)
    similarities = np.dot(doc_embeddings, query_emb)

    # Indices des meilleurs passages
    best_idx = np.argsort(similarities)[::-1][:top_k]

    results = []
    for idx in best_idx:
        results.append({
            "document_index": idx,
            "score": float(similarities[idx]),
            "content": documents[idx].strip()
        })
    return results

print(" Fonction de recherche s√©mantique pr√™te.")

# ============================================================
# 2.4 ‚Äî Tests du moteur de recherche s√©mantique
# ============================================================

queries = [
    "C'est quoi le NLP ?",
    "Comment fonctionne un Transformer ?",
    "Comment entra√Æner un mod√®le de classification ?",
    "Explique le RAG.",
    "Qu'est-ce qu'un chatbot ?"
]

for q in queries:
    print("\n==============================")
    print(" Requ√™te :", q)
    results = search(q, top_k=2)
    for r in results:
        print(f"\n Score : {r['score']:.3f}")
        print(f" Passage : {r['content']}")

# ============================================================
# 2.4 ‚Äî Tests du moteur de recherche s√©mantique
# ============================================================

queries = [
    "C'est quoi le NLP ?",
    "Comment fonctionne un Transformer ?",
    "Comment entra√Æner un mod√®le de classification ?",
    "Explique le RAG.",
    "Qu'est-ce qu'un chatbot ?"
]

for q in queries:
    print("\n==============================")
    print(" Requ√™te :", q)
    results = search(q, top_k=2)
    for r in results:
        print(f"\n Score : {r['score']:.3f}")
        print(f" Passage : {r['content']}")

# ============================================================
# 3.1 ‚Äî Chargement d'un mod√®le g√©n√©ratif (Flan-T5)
# ============================================================

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

gen_model_name = "google/flan-t5-small"  # on peut passer √† flan-t5-base plus tard
tokenizer = AutoTokenizer.from_pretrained(gen_model_name)
gen_model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name)

gen_model = gen_model.to(device)

print(f" Mod√®le g√©n√©ratif charg√© : {gen_model_name}")

# ============================================================
# 3.2 ‚Äî Construction du prompt RAG
# ============================================================

def build_rag_prompt(question, retrieved_passages):
    """
    Construit un prompt texte pour le mod√®le g√©n√©ratif,
    en incluant les documents r√©cup√©r√©s.
    """
    context = ""
    for i, passage in enumerate(retrieved_passages):
        context += f"[DOC {i+1}] {passage['content']}\n"

    prompt = (
        "Tu es un assistant qui r√©pond uniquement √† partir des documents fournis.\n"
        "Si l'information n'est pas pr√©sente dans les documents, dis que tu ne sais pas.\n\n"
        "Documents :\n"
        f"{context}\n"
        "Question : "
        f"{question}\n\n"
        "R√©ponse en fran√ßais clair et p√©dagogique :"
    )
    return prompt

# ============================================================
# 3.3 ‚Äî Fonction de Question-R√©ponse RAG
# ============================================================

def rag_answer(question, top_k=3, max_new_tokens=128):
    """
    1. Retrieve des passages pertinents √† partir des documents.
    2. Construit un prompt incluant ces passages.
    3. Utilise un mod√®le g√©n√©ratif (Flan-T5) pour produire une r√©ponse.
    4. Retourne (passages_utilis√©s, r√©ponse).
    """
    # 1) R√©cup√©ration des passages
    retrieved = search(question, top_k=top_k)

    # 2) Construction du prompt
    prompt = build_rag_prompt(question, retrieved)

    # 3) G√©n√©ration
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False  # greedy pour la p√©dagogie (comportement d√©terministe)
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    return retrieved, answer

# ============================================================
# 3.4 ‚Äî Tests du mini syst√®me RAG
# ============================================================

test_questions = [
    "Explique ce qu'est le NLP.",
    "Comment fonctionne un mod√®le Transformer ?",
    "C'est quoi un embedding de phrase ?",
    "Qu'est-ce que le RAG en intelligence artificielle ?",
    "Comment entra√Æne-t-on un mod√®le de classification de texte ?",
    "Qu'est-ce qu'un chatbot intelligent dans ce contexte ?"
]

for q in test_questions:
    print("\n" + "="*70)
    print("‚ùì Question :", q)

    passages, answer = rag_answer(q, top_k=3)

    print("\n Passages utilis√©s :")
    for i, p in enumerate(passages):
        print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
        print(p["content"])

    print("\n R√©ponse RAG :")
    print(answer)

# ============================================================
# 4.2 ‚Äî Construction du prompt conversationnel
# ============================================================

def build_chat_prompt(history, question, retrieved_passages, max_history=3):
    """
    Construit un prompt incluant :
    - un historique (dernier max_history tours),
    - les documents pertinents,
    - la question actuelle.
    """

    # 1) Historique compact (fen√™tre glissante)
    hist_text = ""
    for (q, a) in history[-max_history:]:
        hist_text += f"Utilisateur : {q}\nAssistant : {a}\n"

    # 2) Documents retrouv√©s
    docs_text = ""
    for i, doc in enumerate(retrieved_passages):
        docs_text += f"[DOC {i+1}] {doc['content']}\n"

    # 3) Construction finale du prompt
    prompt = (
        "Tu es un assistant p√©dagogique qui r√©pond uniquement √† partir des documents fournis.\n"
        "Si l'information n'est pas dans les documents, dis que tu ne sais pas.\n\n"
        f"Historique (contexte conversationnel) :\n{hist_text}\n"
        f"Documents pertinents :\n{docs_text}\n"
        f"Question de l'utilisateur : {question}\n\n"
        "R√©ponse en fran√ßais :"
    )
    return prompt

# ============================================================
# 4.3 ‚Äî Fonction de Chatbot RAG complet
# ============================================================

def chatbot_rag_turn(history, question, top_k=3, max_new_tokens=128):
    """
    Effectue un tour de conversation :
    - retrieve
    - prompt
    - g√©n√©ration
    - mise √† jour de l'historique
    """

    # 1) Retrieval
    retrieved = search(question, top_k=top_k)

    # 2) Prompt complet (historique + docs)
    prompt = build_chat_prompt(history, question, retrieved)

    # 3) G√©n√©ration
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    output = gen_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    # 4) Mise √† jour de l'historique
    history.append((question, answer))

    return answer, retrieved, history

# ============================================================
# 4.4 ‚Äî Boucle interactive du chatbot RAG
# ============================================================

def chat():
    print("ü§ñ Chatbot RAG (tape 'quit' ou 'exit' pour arr√™ter)\n")
    history = []

    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "stop"]:
            print("Assistant : Au revoir ! üëã")
            break

        answer, retrieved, history = chatbot_rag_turn(history, user_input)

        print("\nAssistant :", answer)
        print("\nüìö Passages utilis√©s :")
        for i, p in enumerate(retrieved):
            print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
            print(p["content"])
        print("\n" + "-"*70)

# chat()  # D√©commenter pour tester en local ou dans un terminal

# ============================================================
# 4.A ‚Äî Interface simple : RAG "one-shot" (sans historique)
# ============================================================

def ask_rag(question, top_k=3):
    """
    Pose une question au syst√®me RAG (sans historique de chat).
    Affiche directement la r√©ponse et les passages utilis√©s.
    """
    passages, answer = rag_answer(question, top_k=top_k)

    print("‚ùì Question :", question)
    print("\nüí¨ R√©ponse RAG :")
    print(answer)

    print("\nüìö Passages utilis√©s :")
    for i, p in enumerate(passages):
        print(f"\n[DOC {i+1}] (score={p['score']:.3f})")
        print(p["content"])
    print("\n" + "-"*70)

# ============================================================
# Exemple de test "one-shot"
# ============================================================

query = "Explique le RAG."

print("\n==============================")
print(" Requ√™te :", query)

results = search(query, top_k=2)

for r in results:
    print(f"\n Score : {r['score']:.3f}")
    print(f" Passage : {r['content']}")